"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[927],{1348:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"rag-fundamentals","title":"RAG Fundamentals","description":"RAG (Retrieval-Augmented Generation) is a technique that combines information retrieval with text generation to create AI systems that can answer questions based on specific knowledge sources, rather than relying solely on pre-trained model knowledge.","source":"@site/docs/rag-fundamentals.md","sourceDirName":".","slug":"/rag-fundamentals","permalink":"/Agentic-Book/docs/rag-fundamentals","draft":false,"unlisted":false,"editUrl":"https://github.com/Demolinator/Agentic-Book/tree/main/docs/rag-fundamentals.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Spec-Driven Development","permalink":"/Agentic-Book/docs/spec-driven-development"},"next":{"title":"Implementation Guide","permalink":"/Agentic-Book/docs/implementation-guide"}}');var r=s(4848),t=s(8453);const l={},o="RAG Fundamentals",a={},c=[{value:"What is RAG?",id:"what-is-rag",level:2},{value:"How RAG Works",id:"how-rag-works",level:2},{value:"Step 1: Knowledge Base Preparation",id:"step-1-knowledge-base-preparation",level:3},{value:"Step 2: Question Processing",id:"step-2-question-processing",level:3},{value:"Step 3: Answer Generation",id:"step-3-answer-generation",level:3},{value:"RAG Architecture",id:"rag-architecture",level:2},{value:"Key Components",id:"key-components",level:2},{value:"1. Embedding Model",id:"1-embedding-model",level:3},{value:"2. Vector Database",id:"2-vector-database",level:3},{value:"3. Chunking Strategy",id:"3-chunking-strategy",level:3},{value:"4. Retrieval Strategy",id:"4-retrieval-strategy",level:3},{value:"5. Generation Model",id:"5-generation-model",level:3},{value:"RAG in This Project",id:"rag-in-this-project",level:2},{value:"Knowledge Base",id:"knowledge-base",level:3},{value:"Retrieval",id:"retrieval",level:3},{value:"Generation",id:"generation",level:3},{value:"Features",id:"features",level:3},{value:"Advantages of RAG",id:"advantages-of-rag",level:2},{value:"1. Accuracy",id:"1-accuracy",level:3},{value:"2. Up-to-Date Information",id:"2-up-to-date-information",level:3},{value:"3. Transparency",id:"3-transparency",level:3},{value:"4. Cost-Effective",id:"4-cost-effective",level:3},{value:"Challenges and Solutions",id:"challenges-and-solutions",level:2},{value:"Challenge 1: Chunking Quality",id:"challenge-1-chunking-quality",level:3},{value:"Challenge 2: Retrieval Quality",id:"challenge-2-retrieval-quality",level:3},{value:"Challenge 3: Context Limits",id:"challenge-3-context-limits",level:3},{value:"Challenge 4: Answer Quality",id:"challenge-4-answer-quality",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"rag-fundamentals",children:"RAG Fundamentals"})}),"\n",(0,r.jsx)(n.p,{children:"RAG (Retrieval-Augmented Generation) is a technique that combines information retrieval with text generation to create AI systems that can answer questions based on specific knowledge sources, rather than relying solely on pre-trained model knowledge."}),"\n",(0,r.jsx)(n.h2,{id:"what-is-rag",children:"What is RAG?"}),"\n",(0,r.jsx)(n.p,{children:"RAG works by:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Retrieving"})," relevant information from a knowledge base (like this book)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Augmenting"})," the AI's context with that retrieved information"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generating"})," answers based on the augmented context"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This allows AI systems to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Answer questions about specific documents or knowledge bases"}),"\n",(0,r.jsx)(n.li,{children:"Provide accurate, up-to-date information"}),"\n",(0,r.jsx)(n.li,{children:"Cite sources for their answers"}),"\n",(0,r.jsx)(n.li,{children:'Avoid "hallucinating" information not in the knowledge base'}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"how-rag-works",children:"How RAG Works"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-knowledge-base-preparation",children:"Step 1: Knowledge Base Preparation"}),"\n",(0,r.jsx)(n.p,{children:"First, the knowledge base (like this book) is processed:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chunking"}),": Text is split into smaller pieces (chunks) of 200-300 tokens"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": Each chunk is converted into a vector (numerical representation) using an embedding model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Storage"}),": Vectors are stored in a vector database (like Qdrant) for fast similarity search"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-2-question-processing",children:"Step 2: Question Processing"}),"\n",(0,r.jsx)(n.p,{children:"When a user asks a question:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Question Embedding"}),": The question is converted into a vector using the same embedding model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Similarity Search"}),": The vector database is searched for chunks with similar vectors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Retrieval"}),": Top N most relevant chunks are retrieved (e.g., top 5 chunks)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-3-answer-generation",children:"Step 3: Answer Generation"}),"\n",(0,r.jsx)(n.p,{children:"The retrieved chunks are used to generate an answer:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context Assembly"}),": Retrieved chunks are combined into a context"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prompt Construction"}),": A prompt is created with the question and context"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generation"}),": An AI language model (like OpenAI's GPT) generates an answer based on the context"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Source Citation"}),": The answer includes references to the source chunks"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"rag-architecture",children:"RAG Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502   User      \u2502\r\n\u2502  Question   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Question       \u2502\r\n\u2502  Embedding      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Vector         \u2502\r\n\u2502  Database       \u2502\u2500\u2500\u2510\r\n\u2502  (Qdrant)       \u2502  \u2502 Similarity Search\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n       \u2502              \u2502\r\n       \u2502              \u2502\r\n       \u25bc              \u2502\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n\u2502  Top N Chunks   \u2502\u25c4\u2500\u2518\r\n\u2502  Retrieved      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Context +      \u2502\r\n\u2502  Question       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  LLM            \u2502\r\n\u2502  (OpenAI GPT)   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Answer +       \u2502\r\n\u2502  Sources        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h2,{id:"key-components",children:"Key Components"}),"\n",(0,r.jsx)(n.h3,{id:"1-embedding-model",children:"1. Embedding Model"}),"\n",(0,r.jsx)(n.p,{children:"Converts text into numerical vectors that capture semantic meaning. Common models:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI text-embedding-3-small"}),": 1536 dimensions, good balance of quality and cost"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI text-embedding-3-large"}),": Higher quality but more expensive"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Open-source alternatives"}),": Sentence-BERT, Instructor, etc."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-vector-database",children:"2. Vector Database"}),"\n",(0,r.jsx)(n.p,{children:"Stores and searches vectors efficiently. Options include:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Qdrant"}),": Fast, scalable, cloud-hosted option"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pinecone"}),": Managed vector database service"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Weaviate"}),": Open-source vector database"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chroma"}),": Lightweight, embedded option"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-chunking-strategy",children:"3. Chunking Strategy"}),"\n",(0,r.jsx)(n.p,{children:"How text is split into chunks affects retrieval quality:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fixed-size chunks"}),": Simple but may split concepts"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Semantic chunking"}),": Splits at natural boundaries"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Overlapping chunks"}),": Ensures context at boundaries"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hierarchical chunks"}),": Multiple sizes for different query types"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"4-retrieval-strategy",children:"4. Retrieval Strategy"}),"\n",(0,r.jsx)(n.p,{children:"How relevant chunks are selected:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Top-K retrieval"}),": Return K most similar chunks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Similarity threshold"}),": Only return chunks above a threshold"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hybrid search"}),": Combine vector search with keyword search"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Re-ranking"}),": Use a second model to re-rank results"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"5-generation-model",children:"5. Generation Model"}),"\n",(0,r.jsx)(n.p,{children:"The language model that generates answers:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI GPT-4"}),": High quality, good reasoning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI GPT-3.5"}),": Faster, lower cost"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Claude"}),": Alternative high-quality model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Open-source LLMs"}),": Llama, Mistral, etc."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"rag-in-this-project",children:"RAG in This Project"}),"\n",(0,r.jsx)(n.p,{children:"This hackathon project implements RAG for answering questions about this book:"}),"\n",(0,r.jsx)(n.h3,{id:"knowledge-base",children:"Knowledge Base"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Source"}),": All markdown files in ",(0,r.jsx)(n.code,{children:"frontend/docs/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chunking"}),": Semantic chunking with 200-300 tokens, 50 token overlap"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embedding"}),": OpenAI text-embedding-3-small (1536 dimensions)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Storage"}),": Qdrant Cloud Free Tier"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"retrieval",children:"Retrieval"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Method"}),": Cosine similarity search"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Top-K"}),": 5 most relevant chunks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Threshold"}),": No minimum threshold (returns top 5)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"generation",children:"Generation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": OpenAI ChatKit SDK (GPT-3.5 or GPT-4)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context"}),": Retrieved chunks + user question"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Answer with optional source citations"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"features",children:"Features"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RAG Mode"}),": Answers questions using the entire book as context"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Selected-Text Mode"}),": Answers questions using only user-selected text (no RAG query)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Source Citations"}),": Shows which chunks were used to generate answers"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"advantages-of-rag",children:"Advantages of RAG"}),"\n",(0,r.jsx)(n.h3,{id:"1-accuracy",children:"1. Accuracy"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Answers are based on actual source material"}),"\n",(0,r.jsx)(n.li,{children:"Reduces hallucinations"}),"\n",(0,r.jsx)(n.li,{children:"Provides verifiable information"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-up-to-date-information",children:"2. Up-to-Date Information"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Knowledge base can be updated without retraining models"}),"\n",(0,r.jsx)(n.li,{children:"New information can be added quickly"}),"\n",(0,r.jsx)(n.li,{children:"Specific to your domain or documents"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-transparency",children:"3. Transparency"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Source citations show where information came from"}),"\n",(0,r.jsx)(n.li,{children:"Users can verify answers"}),"\n",(0,r.jsx)(n.li,{children:"Builds trust in the system"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"4-cost-effective",children:"4. Cost-Effective"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"No need to fine-tune large models"}),"\n",(0,r.jsx)(n.li,{children:"Can use smaller, cheaper models"}),"\n",(0,r.jsx)(n.li,{children:"Update knowledge base without retraining"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"challenges-and-solutions",children:"Challenges and Solutions"}),"\n",(0,r.jsx)(n.h3,{id:"challenge-1-chunking-quality",children:"Challenge 1: Chunking Quality"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": Poor chunking can split important concepts."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Use semantic chunking with overlap, test different chunk sizes."]}),"\n",(0,r.jsx)(n.h3,{id:"challenge-2-retrieval-quality",children:"Challenge 2: Retrieval Quality"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": Relevant chunks might not be retrieved."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Use better embeddings, increase top-K, try hybrid search."]}),"\n",(0,r.jsx)(n.h3,{id:"challenge-3-context-limits",children:"Challenge 3: Context Limits"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": Too many chunks exceed model context limits."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Limit number of chunks, use compression, prioritize most relevant."]}),"\n",(0,r.jsx)(n.h3,{id:"challenge-4-answer-quality",children:"Challenge 4: Answer Quality"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": Model might ignore context or hallucinate."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Use better prompts, enforce context usage, add validation."]}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chunking"}),": Use semantic chunking with overlap for better context"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embeddings"}),": Choose appropriate embedding model for your domain"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Retrieval"}),": Retrieve enough chunks (5-10) but not too many"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prompts"}),": Clearly instruct the model to use only provided context"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Validation"}),": Check that answers are actually in the retrieved chunks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citations"}),": Always provide source citations for transparency"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"RAG is a powerful technique for building AI systems that can answer questions about specific knowledge bases. By combining retrieval with generation, RAG systems can provide accurate, verifiable answers while avoiding hallucinations."}),"\n",(0,r.jsx)(n.p,{children:"This project demonstrates RAG implementation using modern tools (OpenAI, Qdrant, FastAPI) and best practices (semantic chunking, source citations, context validation)."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous"}),": ",(0,r.jsx)(n.a,{href:"/Agentic-Book/docs/spec-driven-development",children:"Spec-Driven Development \u2190"})," | ",(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"/Agentic-Book/docs/implementation-guide",children:"Implementation Guide \u2192"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>o});var i=s(6540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);